{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.9997530305375207\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/Users/reborn/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "predictions = model.transform(dataset)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -785.4888734297592\n",
      "The upper bound on perplexity: 3.0211110612326886\n",
      "The topics described by their top-weighted terms:\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|topic|termIndices|termWeights                                                    |\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "|0    |[0, 1, 6]  |[0.11424911461637872, 0.09782822062211247, 0.09753950669152199]|\n",
      "|1    |[0, 2, 8]  |[0.10209989682242301, 0.10032719649204634, 0.09979252018426654]|\n",
      "|2    |[2, 5, 10] |[0.10076593158165223, 0.09952905578561737, 0.09788412897819268]|\n",
      "|3    |[8, 9, 2]  |[0.10528434420848354, 0.10091066237527938, 0.10003654947255891]|\n",
      "|4    |[10, 9, 6] |[0.19901381426645826, 0.13981330672252715, 0.13653345103101114]|\n",
      "|5    |[3, 0, 4]  |[0.24512343825451446, 0.1132255261983528, 0.11106617831293743] |\n",
      "|6    |[0, 1, 10] |[0.10295223176949261, 0.09868410481798896, 0.09812439429778264]|\n",
      "|7    |[1, 7, 5]  |[0.108333691184128, 0.09839255459795179, 0.09814053237217438]  |\n",
      "|8    |[4, 0, 5]  |[0.16397312462512587, 0.14969804729964145, 0.14243550890035175]|\n",
      "|9    |[8, 1, 6]  |[0.10636483981415729, 0.10392697216003612, 0.09726835392774555]|\n",
      "+-----+-----------+---------------------------------------------------------------+\n",
      "\n",
      "+-----+---------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|features                                                       |topicDistribution                                                                                                                                                                                                        |\n",
      "+-----+---------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0.0  |(11,[0,1,2,4,5,6,7,10],[1.0,2.0,6.0,2.0,3.0,1.0,1.0,3.0])      |[0.004780422389289665,0.004780574511760576,0.004780578132005782,0.004780579546547551,0.5315379452204609,0.00503704921077577,0.004780453187281983,0.0047804139071371985,0.42996162702731544,0.004780356867425289]         |\n",
      "|1.0  |(11,[0,1,3,4,7,10],[1.0,3.0,1.0,3.0,2.0,1.0])                  |[0.007973835264963827,0.007973991205632906,0.007974007433108315,0.007973928638944972,0.008284968024976457,0.008404723801230786,0.007974127992749101,0.007973915723590517,0.9274925414475529,0.007973960467250334]        |\n",
      "|2.0  |(11,[0,1,2,5,6,8,9],[1.0,4.0,1.0,4.0,9.0,1.0,2.0])             |[0.004156095080712102,0.004156062824377832,0.004156029592046727,0.004156079941343328,0.5875835949916919,0.004379034011379154,0.004156044947809805,0.004156126395961894,0.37894487354082973,0.004156058673847563]         |\n",
      "|3.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,3.0,9.0])            |[0.00367594516804643,0.0036759629180440822,0.003675908721241723,0.0036759487722636563,0.9662813479486546,0.003874000580883718,0.0036759518882808974,0.003675933201360501,0.004113056883628746,0.003675943917595523]      |\n",
      "|4.0  |(11,[0,1,2,3,4,6,9,10],[3.0,1.0,1.0,9.0,3.0,2.0,1.0,3.0])      |[0.003982628201746355,0.003982644616276096,0.003982604836904462,0.003982631358823252,0.004137307347857426,0.9635276328275627,0.003982649118805845,0.0039826016944865305,0.0044566990593664,0.0039826009381709404]        |\n",
      "|5.0  |(11,[0,1,3,4,5,6,7,8,9],[4.0,2.0,3.0,4.0,5.0,1.0,1.0,1.0,4.0]) |[0.0036760128728318352,0.0036760112076441986,0.0036759693425940923,0.0036760262264866513,0.003818534817342083,0.0038737318126335248,0.0036759975558870615,0.0036760001673680244,0.9665757307879194,0.0036759852092931398]|\n",
      "|6.0  |(11,[0,1,3,6,8,9,10],[2.0,1.0,3.0,5.0,2.0,2.0,9.0])            |[0.0038231348732643347,0.003823152524562958,0.0038230966244619242,0.00382313488234923,0.964931412736944,0.0040289326910595315,0.0038231421979471355,0.0038231220124845693,0.004277737120878062,0.003823134336048234]     |\n",
      "|7.0  |(11,[0,1,2,3,4,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,1.0,2.0,1.0,3.0])|[0.004345136136218042,0.0043451713886668115,0.004345143751697124,0.0043451628876879905,0.004513899283120014,0.9602074702884946,0.004345161080183036,0.004345140049393812,0.0048626069470371945,0.004345108187501386]     |\n",
      "|8.0  |(11,[0,1,3,4,5,6,7],[4.0,4.0,3.0,4.0,2.0,1.0,3.0])             |[0.004345121170773076,0.004345057958187071,0.004345052792135042,0.004345068479886926,0.004513364424965531,0.004579030413743903,0.004345110116846012,0.004345101961164938,0.9604920195216551,0.004345073160642417]        |\n",
      "|9.0  |(11,[0,1,2,4,6,8,9,10],[2.0,8.0,2.0,3.0,2.0,2.0,7.0,2.0])      |[0.00329541847587417,0.0032954293740226036,0.0032953998247118423,0.0032954477996839713,0.6245839281559992,0.0034723141844288682,0.0032954264717953217,0.0032954195121523916,0.34887579959375337,0.00329541660757835]     |\n",
      "|10.0 |(11,[0,1,2,3,5,6,9,10],[1.0,1.0,1.0,9.0,2.0,2.0,3.0,3.0])      |[0.0041561041984563615,0.004156153296259286,0.004156117514799784,0.004156158198669812,0.00431842063382961,0.9619372014111194,0.004156112927988479,0.0041561303944450305,0.004651540536894206,0.004156060887537961]       |\n",
      "|11.0 |(11,[0,1,4,5,6,7,9],[4.0,1.0,4.0,5.0,1.0,3.0,1.0])             |[0.00478017514597852,0.0047800625042161374,0.004780068343194407,0.004780080182828915,0.004965095953621337,0.005036708497249764,0.0047801322452637185,0.0047801240599971515,0.9565375081742856,0.004780044893364353]      |\n",
      "+-----+---------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/Users/reborn/spark/data/mllib/sample_lda_libsvm_data.txt\")\n",
    "\n",
    "# Trains a LDA model.\n",
    "lda = LDA(k=10, maxIter=10)\n",
    "model = lda.fit(dataset)\n",
    "\n",
    "ll = model.logLikelihood(dataset)\n",
    "lp = model.logPerplexity(dataset)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))\n",
    "\n",
    "# Describe topics.\n",
    "topics = model.describeTopics(3)\n",
    "print(\"The topics described by their top-weighted terms:\")\n",
    "topics.show(truncate=False)\n",
    "\n",
    "# Shows the result\n",
    "transformed = model.transform(dataset)\n",
    "transformed.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Set Sum of Squared Errors = 0.11999999999994547\n",
      "Cluster Centers: \n",
      "[0.1 0.1 0.1]\n",
      "[9.1 9.1 9.1]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"/Users/reborn/spark/data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a bisecting k-means model.\n",
    "bkm = BisectingKMeans().setK(2).setSeed(1)\n",
    "model = bkm.fit(dataset)\n",
    "\n",
    "# Evaluate clustering.\n",
    "cost = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(cost))\n",
    "\n",
    "# Shows the result.\n",
    "print(\"Cluster Centers: \")\n",
    "centers = model.clusterCenters()\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.2581, 0.7419]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9186, 0.0814]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.432, 0.568]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.6766, 0.3234]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labeled.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "    \n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice\n",
    "\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{Param(parent='HashingTF_469f9208551708ffca0f', name='numFeatures', doc='number of features.'): 10,\n",
       "  Param(parent='LogisticRegression_47dcabf8ae648a0e25e2', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='HashingTF_469f9208551708ffca0f', name='numFeatures', doc='number of features.'): 10,\n",
       "  Param(parent='LogisticRegression_47dcabf8ae648a0e25e2', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='HashingTF_469f9208551708ffca0f', name='numFeatures', doc='number of features.'): 100,\n",
       "  Param(parent='LogisticRegression_47dcabf8ae648a0e25e2', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='HashingTF_469f9208551708ffca0f', name='numFeatures', doc='number of features.'): 100,\n",
       "  Param(parent='LogisticRegression_47dcabf8ae648a0e25e2', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='HashingTF_469f9208551708ffca0f', name='numFeatures', doc='number of features.'): 1000,\n",
       "  Param(parent='LogisticRegression_47dcabf8ae648a0e25e2', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='HashingTF_469f9208551708ffca0f', name='numFeatures', doc='number of features.'): 1000,\n",
       "  Param(parent='LogisticRegression_47dcabf8ae648a0e25e2', name='regParam', doc='regularization parameter (>= 0).'): 0.01}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(paramGrid))\n",
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Param(parent='CrossValidator_4fd4b58569c45ea4b494', name='estimator', doc='estimator to be cross-validated'),\n",
       " Param(parent='CrossValidator_4fd4b58569c45ea4b494', name='estimatorParamMaps', doc='estimator param maps'),\n",
       " Param(parent='CrossValidator_4fd4b58569c45ea4b494', name='evaluator', doc='evaluator used to select hyper-parameters that maximize the validator metric'),\n",
       " Param(parent='CrossValidator_4fd4b58569c45ea4b494', name='numFolds', doc='number of folds for cross validation'),\n",
       " Param(parent='CrossValidator_4fd4b58569c45ea4b494', name='parallelism', doc='the number of threads to use when running parallel algorithms (>= 1).'),\n",
       " Param(parent='CrossValidator_4fd4b58569c45ea4b494', name='seed', doc='random seed.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            features|               label|          prediction|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|(10,[0,1,2,3,4,5,...|  -23.51088409032297| -1.6659388625179559|\n",
      "|(10,[0,1,2,3,4,5,...| -21.432387764165806|  0.3400877302576284|\n",
      "|(10,[0,1,2,3,4,5,...| -12.977848725392104|-0.02335359093652395|\n",
      "|(10,[0,1,2,3,4,5,...| -11.827072996392571|  2.5642684021108417|\n",
      "|(10,[0,1,2,3,4,5,...| -10.945919657782932| -0.1631314487734783|\n",
      "|(10,[0,1,2,3,4,5,...|  -10.58331129986813|   2.517790654691453|\n",
      "|(10,[0,1,2,3,4,5,...| -10.288657252388708| -0.9443474180536754|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.822357870425154|  0.6872889429113783|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.772667465932606|  -1.485408580416465|\n",
      "|(10,[0,1,2,3,4,5,...|  -8.605713514762092|   1.110272909026478|\n",
      "|(10,[0,1,2,3,4,5,...|  -6.544633229269576|  3.0454559778611285|\n",
      "|(10,[0,1,2,3,4,5,...|  -5.055293333055445|  0.6441174575094268|\n",
      "|(10,[0,1,2,3,4,5,...|  -5.039628433467326|  0.9572366607107066|\n",
      "|(10,[0,1,2,3,4,5,...|  -4.937258492902948|  0.2292114538379546|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.741044592262687|   3.343205816009816|\n",
      "|(10,[0,1,2,3,4,5,...|  -3.731112242951253| -2.6826413698701064|\n",
      "|(10,[0,1,2,3,4,5,...|  -2.109441044710089| -2.1930034039595445|\n",
      "|(10,[0,1,2,3,4,5,...| -1.8722161156986976| 0.49547270330052423|\n",
      "|(10,[0,1,2,3,4,5,...| -1.1009750789589774| -0.9441633113006601|\n",
      "|(10,[0,1,2,3,4,5,...|-0.48115211266405217| -0.6756196573079968|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data.\n",
    "data = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"/Users/reborn/spark/data/mllib/sample_linear_regression_data.txt\")\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "model.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
